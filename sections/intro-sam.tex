\section{Introduction}
\label{sec:intro}

Mobile devices, edge-, and cloud-computing have the potential to form a \textit{computing continuum} on which new and disruptive types of applications can be built. This continuum enables the seamless convergence of heterogeneous infrastructure, stretching all the way from cloud resources to mobile devices, including intermediate steps such as ISP gateways, cellular base stations, and private cloud deployments.

The heterogeneity of the computing continuum is profound and multi-faceted. In the cloud, computing resources are typically provided through virtualization and containerization~\cite{leitner2016patterns, Quatrocchi2016discrete}, and there is an illusion of infinite resource availability thanks to horizontal scaling. In contrast, in edge computing, computational resources are scarce and must be managed very efficiently~\cite{Dehos14millimeter5g,GarrigaMendonca2017}. This is even truer for mobile devices, as they are strongly constrained by battery and other limitations. 

In the cloud, networking protocols and technologies (e.g., traffic managers, DNS, etc.) allow clients to access resources across countries and continents. Conversely, in edge computing, to access resources they must be available within the client's network coverage, be it cellular (e.g., 5G) or local (e.g., domestic or office). Of course, we can consider the computational resources of the client's mobile device to be always accessible. % as long as the battery lasts.

Finally, there are important QoS considerations that need to be made. While cloud resources can provide vast computing power through elasticity, accessing them may involve multiple hops of network communication, leading to prohibitive latency in the processing of client requests. Indeed, one of the main motivations for introducing edge computation is to mitigate the network latency~\cite{Shi:2016}, which is nullified when execution is performed locally on the client's mobile device.

The advantage of embracing the computing continuum is that it allows an application to situationally and opportunistically decide where in the continuum a specific calculation should be performed. This decision will consider the resources that are actually available in the continuum at that specific moment in time, depend on the client's geo-location and connectivity, and be informed by existing QoS requirements, e.g., maximum acceptable latency~\cite{GuptaIfogSim17}. Given that the providers are not able to autonomously coordinate and decide who should serve a client's request, the decision logic is shared with the client's device, so that the best alternative can be selected every time.

In this paper, we propose a unified model for the computing continuum called A3-E. At its heart A3-E proposes a model for the automated management of a software service's life-cycle. The model takes its name from its four main phases: \textit{(A)wareness, (A)cquisition, (A)llocation} and \textit{(E)ngagement}. First of all, the model enables a mutual client-provider awareness that allows for the dynamic placement of stateless computation along the continuum. Second, the model extends the Functions-as-a-Service (FaaS) computing paradigm~\cite{Hendrickson:2016,baldini2017serverless,GarrigaMendonca2017} to allow stateless functions to be autonomously fetched, deployed and exposed --as microservices-- by a provider. 

%TODO I replaced the client-side and provider-side middleware by just middleware, to avoid both the inconsistency among provider and domain (used latter), while presenting the middleware as a whole.
In this paper we also present a middleware that helps realize the \textit{A3-E model}. The middleware is distributed across the continuum, and conceptually divided into two parts: one is responsible for facilitating the development and provisioning of self-managed microservice life-cycles, while the other is responsible for handling application requests and forwarding them to the provider that can best satisfy the client's requirements.


%In this paper we also present a \textit{client-side middleware} and a \textit{provider-side middleware} that help realize the \textit{A3-E model}. The former is responsible for handling application requests and forwarding them to the provider that can best satisfy the client's requirements; while the latter facilitates the development and provisioning of self-managed service life-cycles. 

A3-E has been evaluated in the context of an Image Recognition application. Thanks to A3-E the application was able to autonomously proxy its requests to services that were dynamically selected from a computing continuum. In our experiments the continuum was composed of a mobile runtime, two local-edge servers, and a cloud environment. The experiments show up to a $90$\% reduction of latency when edge services are used instead of cloud services, and a $74$\% decrease of battery consumption when computation is offloaded from the Android device to edge/cloud servers. Moreover, by dynamically selecting what constituent to use in the continuum we were able to obtain 100\% availability; simultaneously we reduced the overall execution time, measured when only using the cloud; and the battery consumption, measured when only using the mobile runtime.

The rest of this paper is organized as follows. Section~\ref{sec:example} discusses the challenges of developing applications in the continuum, and presents a running example scenario. Section~\ref{sec:proposal} provides a detailed description of the A3-E model, whereas Section~\ref{sec:implementation} details the implementation of client-side and server-side middlewares. Section~\ref{sec:evaluation} reports on the experiments performed to evaluate our proposal. Section~\ref{sec:related} presents related work. Finally, Section~\ref{sec:conclusions} concludes the paper and delineates future work.

