\section{Related Work}
\label{sec:related}


The first notion of a computational continuum was based on cloudlets (trusted, resource-rich computer or cluster of computers, well-connected to the Internet and available nearby mobile devices~\cite{Satyanarayanan09cloudlets}) for a Mobile-Cloudlet-Cloud architecture, capable of reducing response time by offloading real-time face recognition facilities~\cite{Soyata:2012}. From a theoretical point of view, in~\cite{sarkar2015assessment} authors compared the performance of the continuum (bridged by means of fog computing) and cloud computing in the context of IoT, by mathematically formulating the parameters and characteristics of each one. Their experiments show that the continuum outperforms cloud computing as the fraction of applications demanding low-latency response increases. However, in scenarios where many applications do not have real-time response requirements, the continuum is observed to be an overhead compared to cloud-only execution.


%\textcolor{blue}{[Gupta IfogSim]}



%\subsection{Edge and Mobile Edge}
The work in~\cite{beck2014mobile} provides technical details of a real-world mobile-edge prototype platform (by Nokia and Intel), together with a taxonomy of applications that can leverage such a platform. In line with our work, mobile-edge servers on base stations are equipped with computational capabilities and application deployment is based on virtualization technologies. Applications running on the mobile edge are expected to be event-driven, as in the serverless model. Moreover, identified use cases encompass offloading and local connectivity, both aspects covered by A3-E. 

Ismail et al.~\cite{ismail2015icos} evaluated different aspects of the deployment and operation of a container technology locally on edge nodes. In their work, a testbed was setup using a database and three edge nodes interconnected by a company network. Despite the similarity with this work, our proposal goes beyond virtualization and containerization of application logic, in favor of serverless computing to optimize the use of edge resources.

The work in~\cite{Satria2017mec} proposes computation offloading among neighbor mobile-edge nodes either directly or using nearby devices as relay nodes. 
In a similar direction, T\"arneberg et al.~\cite{Tarneberg2017} proposed a model that materializes the continuum through mobile edge computing and the distributed cloud paradigm, as well as an algorithm for application placement and resource management across the topology. Our proposal can be seen as complementary, since we envision an heterogeneous continuum, in which edge nodes may not be aware of each other nor able to coordinate computation offloading (e.g., local edge nodes may not be connected with mobile ones), and also their availability depend on what is perceived by the client. For this reason, offloading is dynamically decided by A3-E on the client side according to the perceived QoS of the available domains. 
%Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec} or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.

%mitigates the overload in MEC servers by deploying a serverless architecture on them, which provides an effective and efficient usage of available resources.  As we envision an heterogeneous continuum in which edge nodes may not be aware of each other (e.g., a domestic edge node may not be connected with a mobile one).

%. One recovery scheme is where an overloaded MEC server offloads its work to available neighbors within transfer range. The other recovery scheme is for situations when there is no available neighboring MEC within transfer range, and uses devices as ad-hoc relay nodes in order to bridge two MEC servers. 

%\subsection{Serverless Architectures}

%IBM OpenWhisk is an open source experimental FaaS started in 2016. It provides a programming model to upload event handlers to a cloud service, and register the handlers to respond to various events or direct invocations from web/mobile apps or other endpoints.

%Google Cloud Function is another service started in 2016 and still in alpha-release. Functions can be triggered by any Google service that supports cloud  pub/sub, cloud storage, arbitrary web hooks or direct triggers.

%Microsoft Azure Functions supports integration with other Azure products, plus timers and arbitrary applications via webhooks or HTTP triggers, and on-premises via the service bus.

%Webtask.io supports scheduled tasks in addition to events, but as a stand-alone tool – those events arrive via webhooks and HTTP endpoints. Webtask.io is built atop CoreOS, Docker, etcd and fleet, with bunyan and Kafka for logging.

%Finally, OpenLambda is an academic effort towards an open-source serverless architecture~\cite{hendrickson2016serverless}, consist of a number of subsystems that coordinate to run Lambda handlers, including a local execution engine that sandboxes handlers, a load balancer, and a distributed database. 

%The idea of bringing serverless capabilities to the edge is very recent. The first documented efforts date form 2017, and come mostly from the industry. 

From the industry point of view, Lambda@Edge\footnote{http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html} is a new functionality of AWS that allows one to explicitly deploy lambda functions to certain edge locations, according to their CloudFront content delivery service, which consists of approximately 90 edge nodes worldwide. However, the functionality covered by Lambda@Edge is still limited to a few scenarios, all of them tied to CloudFront events. In contrast, we consider the continuum to be fine-grained, with nearby domains distributed one every $km^2$ or less. Furthermore, the upcoming  small 5G cells and microcells~\cite{beck2014mobile} allow us to think of one local-edge domain per block, or even per building in certain vital places, such as government buildings or transport stations.

EdgeScale~\cite{Lara2016hierarchical} is a recently proposed platform that leverages serverless cloud computing to enable storage and processing on a hierarchy of data centers, positioned over the geographic span of a network between the user and traditional wide-area cloud providers. EdgeScale implements all the functions, storage, routing and additional capabilities from scratch, while we opted for leveraging current open technologies such as IBM/Apache Openwhisk, which have broad support from a major vendor and an active community Apache. Besides, EdgeScale is on an early stage and does not report any empirical evaluation of latency, throughput and bandwidth.

\textcolor{blue}{ Gartner predicting that smartphones would be the Internet access device of choice by 2018 [25 IFOGSIM Gartner]. }  

\textcolor{blue}{ Nastic et al [NASTIC SERVERLESSEDGE] propose a unified cloud and edge data analytics platform, which extends the notion of serverless computing to the edge via a reference architecture for such platform and a serverless application execution model, which enable uniform development and operation of analytics functions. This approach enables combining the benefits of the edge (lower response time and heterogeneous data management) with the computational and storage capabilities of the cloud.
	The proposed serverless data analytics paradigm is particularly suitable for managing different granularities of data analytics approaches bottom-up. This means that the edge focuses on local views (for example, per edge gateway), while the cloud supports global views, that is, combining and analyzing data from different edge devices, regions, or even domains. Data analytics can be performed on edge nodes, cloud nodes, or both, and delivered from any of the nodes directly to the application, based on the desired view. Moreover, the top-down control process allows decoupling of application requirements (the what) from concrete realization of those requirements (the how). This allows developers to simply define the analytics function behavior and data-processing business logic and application goals  instead of dealing with the complexity of different management, orchestration,
	and optimization processes. }

\textcolor{blue}{ Within this proposal, the \textbf{Serverless Stream Model} an extension of the traditional stream processing model. The transformation function is the core concept and encapsulates user-defined data analytics logic to process data along the stream. These functions are then composed into topologies that enable complex data processing applications.  The wrapper is responsible for encapsulating the transformation functions and exposing a thin API layer, enabling the analytics function layer to treat functions as \textbf{microservices}.  For stateful functions, these wrappers also provide implicit state management. The wrapper transparently handles state replication and migration, and access to a function’s state is controlled via the exposed API. }

\textcolor{blue}{ Wang et al proposed ENORM (A Framework For Edge NOde Resource Management) [Wang ENORM 17], aimed to realise fog computing the challenge of managing edge nodes.  The partitioned server on the edge node is different from the cloud application server in that localised data relevant to the users covered by the edge node is maintained. The global view is maintained on the cloud server and the edge node updates the cloud server. When edge nodes cannot provide computing services (for example, if the edge node is overloaded during peak hours and there are no spare cycles) or the edge nodes cannot improve the QoS of the application, then the deployed edge server will be terminated and users connect to the cloud application server as in the cloud-only execution model.}

\textcolor{blue}{  ENORM framework, uses the following five components on the edge node:
	Resource allocator keeps track of the available CPU cores and memory. Edge Manager is composed by the node manager (deals with the requests obtained by the server manager from a cloud server) and server manager (initializes containers, allocates ports and security). Monitor, since each application edge server is monitored periodically by tracking communication/computing latency (through pings and timestamps on server logs respectively). Auto-scaler dynamically allocates/deallocates hardware resources to the containers executing application servers. Finally, Application Edge Server is the actual server that consists on a partition of the cloud server, hosted on the edge node. } ENORM is particularly focused on resource management (top-down) from cloud to edge. We envision a bottom-up approach in which execution is firstly attempted at edge nodes and then going up in the hierarchy given the unfeasibility of running near to the devices.


