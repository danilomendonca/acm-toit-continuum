\section{Related Work}
\label{sec:related}

%The computational continuum
The first notion of a computing continuum was based on cloudlets (trusted, resource-rich computers or clusters of computers, connected to the Internet and available for nearby mobile devices~\cite{Satyanarayanan09cloudlets}) for a Mobile-Cloudlet-Cloud architecture capable of reducing response time~\cite{Soyata:2012}. Cloudlets are homogeneous and connected to each other to coordinate the offloading of computational intensive tasks. Static data (e.g., template images for face recognition) is stored in the cloudlets beforehand. Our approach encompasses intra-domain allocation (e.g., among cloudlets in the same domain), but also considers heterogeneous and disjoint domains among which a coordinated allocation is not feasible (e.g., among cloudlets from different providers). In the later case, allocation is performed dynamically by clients given the perceived QoS from different domains.
%TODO [Danilo] To claim that inter-domain coordination is not feasible is too strong and biased. Whilst this may be true for different providers, placement among different domains from the same provider is actually the focus of most researchers and related works. We must enphatize our focus on intra-domain service allocation and state that inter-domain placement is part of our future work.

From a theoretical point of view, the work in~\cite{sarkar2015assessment} compared the performance of the continuum (bridged by means of fog computing) and cloud computing in the context of IoT, showing that the continuum outperforms cloud computing as the fraction of applications demanding low-latency response increases. However, in scenarios where many applications do not have real-time response requirements, the continuum is observed to be an overhead compared to cloud-only execution.

A number of works have tackled the problem of placing among the set of computational entities from cloud and edge computing. Some~\cite{Tarneberg2017} focus on the placement of single component applications, whereas others~\cite{Wang:2017} consider multi-components. In comparison, our model allows multiple instances of $\mu$-services to be spread across the continuum and we focus on the opportunistic allocation of instances within disjoint provider \textit{domains}. Nonetheless, the coordination among nearby edge domains may further increase scalability of the edge layer and is considered for future work.

%TODO [Danilo] general placement is mentioned in the previous paragraph; consider adding it back if we have space left
%In a similar direction, authors in~\cite{Tarneberg2017} propose an algorithm for application placement and resource management across an heterogeneous topology comprised of cloud and edge computing. 
%%This approach stresses the challenges for such an heterogeneous topology as meeting all client applicationsâ€™ SLAs, minimizing infrastructure-wide operational expenditure and load balance of resources and mitigate resource usage skewness. 
%The algorithm solves a multi-optimization problem considering application
%execution cost and the overload penalty on each node in terms of latency, as well as edge resources in the system. The objective function is constructed from the infrastructure provider's viewpoint (i.e., Telecom operators), whose objective is to minimize the overall running cost~\cite{weber2017facilitating}. We also envision a heterogeneous continuum, though we consider autonomous and independent \textit{domains} that cannot communicate and coordinate for resource allocation. The proposed placement algorithm complements A3-E, as it could be employed by providers of multiple domains to optimize their costs (e.g., by restricting the availability of resources at different mobile-edge domains in certain conditions). In turn, A3-E tackles the adaptation to the conditions imposed by providers (e.g., with the deallocation of low-priority microservices from these domains and the consequent client-side selection of alternative domains).

More closely related to our service model, Jia et al.~\cite{Jia:2017} tackled the offloading of  virtualized network functions (NVF) to distributed cloudlets. Whilst this proposal focuses on NFV placement among cloudlets from a provider viewpoint, our model deals with the dynamic allocation of $\mu$-services on a single domain's pool of resources, as well as the runtime selection of mobile, edge, and cloud domains as perceived by the client. %Once more, provider-side placement within the edge layer (e.g., cloudlets) is considered as future work. 

The Enorm framework~\cite{wang2017enorm} for edge node resource management exhibits similarities with our model. In their proposal, cloud managers are responsible for the allocation of server-side application partitions on edge nodes with the aim of reducing service latency and the volume of data sent to the cloud. Server allocation follows a handshake between cloud and edge managers. If successful, servers are deployed and clients are binded to specific edge node ports following a network-level reconfiguration. In contrast, in A3-E $\mu$-services are dynamically deployed after a direct handshake between clients and autonomous domains, including those of mobile, edge and cloud providers; and clients decide on the best domain given their requirements and perceived QoS. Like in Enorm, our model takes into account both priority and latency throughout Allocation. Nonetheless, in A3-E requests from different clients are not pre-allocated to specific instances, but handled by a load balancer. Also, A3-E's resource management is based on aggregated metrics such as the average response time and follows a control theoretical model able to deal with a high workload churn and a large number of concurrent requests. 

% [Danilo] from Enorm; commented-out in favor of a more comparative discussion
%This server communicates with potential edge nodes requesting computing services and deploys partitioned servers on the edge node. The latter maintain local data, relevant to the users on their coverage area. The global view is maintained on the cloud and updated periodically by edge nodes. When edge nodes cannot provide computing services nor improve the QoS of the application, users connect to the cloud application server as in a typical cloud scenario. The allocation follows a hierarchical approach, with a centralized control starting from the cloud and spreading to the edge nodes. In turn, A3-E offers a decentralized approach in which autonomous clients and domains are able to coordinate the placement of computation along the continuum. 

%TODO Xu et al. 2017

%In this category, Want et al.~\cite{Wang:2017} proposed an online placement approximation of multi-component applications as a graph placement problem. The proposal considers computational resources of physical nodes and the capacity of network links. Each application components may be placed on different entities, but instances of the same component are collocated.  Also, it focuses on the opportunistic allocation of instances within a domain boundary. Inter-domain placement could

More recently, iFogSim~\cite{GuptaIfogSim17} was proposed as a toolkit for modeling and simulating resource management techniques in the continuum. It allows one to simulate the placement of different modules of real-time applications to edge devices and then measure latency, network congestion, energy consumption, and cost. Similarly, EdgeCloudSim~\cite{SonmezEgdeCloudSim17} simulates multi-tier scenarios where multiple edge servers are running in coordination with upper layer cloud solutions, also featuring a device mobility model and a load generator. Both simulation platforms extend the well-known cloud simulator framework CloudSim~\cite{calheiros2011cloudsim}. We opted for deploying real edge domains rather than simulating them, although simulation of mobile-edge scenarios is considered as a future work.

Nowadays, realizing edge computing is seen as the novel and challenging part of the continuum. In~\cite{Nastic17ServerlessDataAnalytics}, a unified cloud and edge data analytics platform is proposed. It extends the notion of serverless computing to the edge via a reference architecture, enabling uniform development and operation of analytics functions, hiding the heterogeneity from the client. At the hearth of the model lays an orchestrator that performs a control loop. It receives the application configuration directives, in terms of high-level objectives such as optimizing network latency, and decides how to orchestrate the underlying resources, as well as the user-defined functions, by invoking
the underlying runtime mechanisms. However, the concrete mechanisms for orchestration are left open. In contrast, the A3-E model tackles specific details regarding the autonomous self-management of  microservices along the continuum. Additionally, this approach particularly targets data analytics applications, while our model focuses on general purpose applications (particularly those with low latency requirements). Moreover, prototypes of A3-E middlewares have been implemented and evaluated. 

%This approach enables combining the benefits of the edge (lower response time and heterogeneous data management) with the computational and storage capabilities of the cloud. The proposed serverless data analytics paradigm is particularly suitable for managing different granularities of data analytics approaches bottom-up. This means that the edge focuses on local views (for example, per edge gateway), while the cloud supports global views, that is, combining and analyzing data from different edge devices, regions, or even domains. Data analytics can be performed on edge nodes, cloud nodes, or both, and delivered from any of the nodes directly to the application, based on the desired view. Moreover, the top-down control process allows decoupling of application requirements (the what) from concrete realization of those requirements (the how). This allows developers to simply define the analytics function behavior and data-processing business logic and application goals  instead of dealing with the complexity of different management, orchestration, and optimization processes.

%Computation offloading to the edge
% [Danilo] Out of space, not our main focus
%The work in~\cite{Satria2017mec} proposes concrete mechanisms for computation offloading among nodes located at neighbor base stations. This work provides a reactive domain-side allocation scheme targeting fault-tolerance in which mobile-edge domains are modeled as the set of servers comprised in neighbor base stations. Differently from our model, this work considers mobile devices as means for relaying data from disconnected base stations. In A3-E, the disconnected computational resources are considered as different domains. However, a client-side selection is responsible for reallocating domains based on their availability and provided QoS. 

%either directly or using nearby devices as relay nodes, as a solution for overloaded edge nodes. The former is where a node offloads its work to available neighbors within transfer range, while the latter is where no neighbor nodes within transfer range are available, and uses devices as ad-hoc relay nodes in order to bridge two edge servers. In contrast, our approach mitigates the overload in the edge by first, a serverless architecture that provides an effective and efficient usage of available resources, and second by sensing the QoS in the client-side middleware to decide whether to offload the computation to the edge/cloud. Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec}.

%In a similar direction, T\"arneberg et al.~\cite{Tarneberg2017} proposed a model that bridges mobile edge computing and the distributed cloud paradigm, as well as an algorithm to solve the resource management challenges that emerge from this integration. I or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.



%be seen as a first step to decide whether to use proactive or reactive allocation for each domain and application. From then on, the availability also depends on what is perceived by the clients, thus the placement is coordinated through the middleware. 



%, without depending necessarily on the cloud to decide the offloading. %Although the data management aspect on Enorm can complement our approach, it does not consider 
%and can be complementary to our work by addressing data locality and replication problems at architectural level sinc
%For this reason, offloading is dynamically decided by A3-E on the client side according to the perceived QoS of the available domains. 
%\textcolor{blue}{[Gupta IfogSim]}


% [Danilo] commented-out due to lack of space; consider adding it back if there's space left
%In industry the first real-world prototype of the mobile edge~\cite{beck2014mobile} (by Nokia Siemens and Intel~\cite{NokiaMEC13}) features base stations equipped with commodity hardware, and application deployment is based on virtualization and containerization technologies~\cite{ismail2015icos}. Applications running on the mobile edge are expected to be event-driven, which complements the A3-E model that currently relies on request/response RESTful calls. However, this approach does not take into account the inherent resource limitations of edge computing, tackled by A3-E by adhering to the FaaS execution model.
%%This industry prototype suggests that end users
%%benefit mostly from reduced communication delay, benefiting computation intensive applications by means of offloading (as shown throughout this paper). Meanwhile, Telecom providers benefit from bandwidth consumption reduction and better scalability, while application providers profit with respect to faster
%%services.

% [Danilo] commented-out due to lack of space; consider adding it back if there's space left
%Finally, Lambda@Edge\footnote{http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html} is a new functionality of AWS that allows one to explicitly deploy lambda functions to certain (coarse-grained) edge locations. The applicability of Lambda@Edge is still limited to a few scenarios and tied to CloudFront (the AWS content delivery network) events. The functionality that can be deployed is also limited, allowing only to analyze and modify HTTP headers generated by those events (e.g., to perform redirection and forwarding of requests). Thus, Lambda@Edge is still unsuitable for general purpose applications as those tackled by A3-E. 





% OLD related works, commented-out in the original submission

%In turn, A3-E deals with heterogeneous and general purpose applications and is not tied to any particular source of events.
%, that is, when a user request to see a given content, when the request is forwarded to the cloud (because such content was not available at the edge location), and vice versa for the response.

%EdgeScale~\cite{Lara2016hierarchical}, by Huawei, also brings the notion of serverless to the edge. It implements serverless functions, storage, routing and additional capabilities from scratch (while we opted for current open technologies such as Apache Openwhisk) in a hierarchy of edge data centers, over a network between the user and traditional wide-area cloud providers. Once again, this proposal does not consider independent (and potentially disjoint) domains. Besides, EdgeScale is on an early stage and does not report any empirical evaluation of latency, throughput, bandwidth and cost.

%

%according to their CloudFront content delivery service, which consists of approximately 90 edge nodes worldwide. However,  In contrast, we consider the continuum to be fine-grained, with nearby domains distributed one every $km^2$ or less. Furthermore, the upcoming  small 5G cells and microcells~\cite{beck2014mobile} allow us to think of one local-edge domain per block, or even per building in certain vital places, such as government buildings or transport stations.




%\subsection{Edge and Mobile Edge}
%The work in~\cite{beck2014mobile} provides technical details of a real-world mobile-edge prototype platform (by Nokia and Intel), together with a taxonomy of applications that can leverage such a platform. In line with our work, mobile-edge servers on base stations are equipped with computational capabilities and application deployment is based on virtualization technologies. Applications running on the mobile edge are expected to be event-driven, as in the serverless model. Moreover, identified use cases encompass offloading and local connectivity, both aspects covered by A3-E. 

%Ismail et al.~\cite{ismail2015icos} evaluated different aspects of the deployment and operation of a container technology locally on edge nodes. In their work, a testbed was setup using a database and three edge nodes interconnected by a company network. Despite the similarity with this work, our proposal goes beyond virtualization and containerization of application logic, in favor of serverless computing to optimize the use of edge resources.

%Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec} or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.

%mitigates the overload in MEC servers by deploying a serverless architecture on them, which provides an effective and efficient usage of available resources.  As we envision an heterogeneous continuum in which edge nodes may not be aware of each other (e.g., a domestic edge node may not be connected with a mobile one).

%. One recovery scheme is where an overloaded MEC server offloads its work to available neighbors within transfer range. The other recovery scheme is for situations when there is no available neighboring MEC within transfer range, and uses devices as ad-hoc relay nodes in order to bridge two MEC servers. 

%\subsection{Serverless Architectures}

%IBM OpenWhisk is an open source experimental FaaS started in 2016. It provides a programming model to upload event handlers to a cloud service, and register the handlers to respond to various events or direct invocations from web/mobile apps or other endpoints.

%Google Cloud Function is another service started in 2016 and still in alpha-release. Functions can be triggered by any Google service that supports cloud  pub/sub, cloud storage, arbitrary web hooks or direct triggers.

%Microsoft Azure Functions supports integration with other Azure products, plus timers and arbitrary applications via webhooks or HTTP triggers, and on-premises via the service bus.

%Webtask.io supports scheduled tasks in addition to events, but as a stand-alone tool â€“ those events arrive via webhooks and HTTP endpoints. Webtask.io is built atop CoreOS, Docker, etcd and fleet, with bunyan and Kafka for logging.

%Finally, OpenLambda is an academic effort towards an open-source serverless architecture~\cite{hendrickson2016serverless}, consist of a number of subsystems that coordinate to run Lambda handlers, including a local execution engine that sandboxes handlers, a load balancer, and a distributed database. 

%The idea of bringing serverless capabilities to the edge is very recent. The first documented efforts date form 2017, and come mostly from the industry. 

%\textcolor{blue}{ Gartner predicting that smartphones would be the Internet access device of choice by 2018 [25 IFOGSIM Gartner]. }  

%\textcolor{blue}{  }

%\textcolor{blue}{ Within this proposal, the \textbf{Serverless Stream Model} an extension of the traditional stream processing model. The transformation function is the core concept and encapsulates user-defined data analytics logic to process data along the stream. These functions are then composed into topologies that enable complex data processing applications.  The wrapper is responsible for encapsulating the transformation functions and exposing a thin API layer, enabling the analytics function layer to treat functions as \textbf{microservices}.  For stateful functions, these wrappers also provide implicit state management. The wrapper transparently handles state replication and migration, and access to a functionâ€™s state is controlled via the exposed API. }


%\textcolor{blue}{  ENORM framework, uses the following five components on the edge node:	Resource allocator keeps track of the available CPU cores and memory. Edge Manager is composed by the node manager (deals with the requests obtained by the server manager from a cloud server) and server manager (initializes containers, allocates ports and security). Monitor, since each application edge server is monitored periodically by tracking communication/computing latency (through pings and timestamps on server logs respectively). Auto-scaler dynamically allocates/deallocates hardware resources to the containers executing application servers. Finally, Application Edge Server is the actual server that consists on a partition of the cloud server, hosted on the edge node. } ENORM is particularly focused on resource management (top-down) from cloud to edge. We envision a bottom-up approach in which execution is firstly attempted at edge nodes and then going up in the hierarchy given the unfeasibility of running near to the devices.


