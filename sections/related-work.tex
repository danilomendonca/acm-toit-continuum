\section{Related Work}
\label{sec:related}

%The computational continuum
The first notion of a computational continuum was based on cloudlets (trusted, resource-rich computer or cluster of computers, well-connected to the Internet and available for nearby mobile devices~\cite{Satyanarayanan09cloudlets}) for a Mobile-Cloudlet-Cloud architecture, capable of reducing response time by offloading real-time face recognition facilities~\cite{Soyata:2012}. From a theoretical point of view,~\cite{sarkar2015assessment} compared the performance of the continuum (bridged by means of fog computing) and cloud computing in the context of IoT, showing that the continuum outperforms cloud computing as the fraction of applications demanding low-latency response increases. However, in scenarios where many applications do not have real-time response requirements, the continuum is observed to be an overhead compared to cloud-only execution.


Recently, efforts are mainly addressing Edge computing, as the novel and challenging part of the continuum.~\cite{Nastic17ServerlessDataAnalytics} proposed a unified cloud and edge data analytics platform, which extends the notion of serverless computing to the edge via a reference architecture, enabling uniform development and operation of analytics functions. This approach particularly targets data analytics applications, and can be complementary to our work by addressing data locality and replication problems at architectural level.

%This approach enables combining the benefits of the edge (lower response time and heterogeneous data management) with the computational and storage capabilities of the cloud. The proposed serverless data analytics paradigm is particularly suitable for managing different granularities of data analytics approaches bottom-up. This means that the edge focuses on local views (for example, per edge gateway), while the cloud supports global views, that is, combining and analyzing data from different edge devices, regions, or even domains. Data analytics can be performed on edge nodes, cloud nodes, or both, and delivered from any of the nodes directly to the application, based on the desired view. Moreover, the top-down control process allows decoupling of application requirements (the what) from concrete realization of those requirements (the how). This allows developers to simply define the analytics function behavior and data-processing business logic and application goals  instead of dealing with the complexity of different management, orchestration, and optimization processes.

%Computation offloading to the edge

~\cite{Satria2017mec} proposes computation offloading among neighbor mobile-edge nodes either directly or using nearby devices as relay nodes. 
In a similar direction,~\cite{Tarneberg2017} proposed an algorithm for application placement and resource management across an heterogeneous topology that resembles the continuum. This proposal benefits Telecom operators as it allows them to optimize usage of servers and network links within the topology, with near-optimal cost. We also envision an heterogeneous continuum, though edge nodes may not be aware of each other nor able to coordinate computation offloading (e.g., local edge nodes may not be connected with mobile ones), and their availability also depends on what is perceived by the client. 

Moreover, the Enorm framework~\cite{wang2017enorm} for edge node resource management  proposes a partitioned server on the edge nodes to maintain local data, relevant to the users covered by each edge node. The global view is then maintained on the cloud, updated periodically by edge nodes. When edge nodes cannot provide computing services nor improve the QoS of the application, users connect to the cloud application server as in a typical cloud scenario.
%For this reason, offloading is dynamically decided by A3-E on the client side according to the perceived QoS of the available domains. 
%\textcolor{blue}{[Gupta IfogSim]}


From the industry point of view, the first real-world prototype~\cite{beck2014mobile} (by Nokia Siemens and Intel) features base stations equipped with commodity hardware, and application deployment is based on virtualization and containerization technologies~\cite{ismail2015icos}. Applications running on the mobile edge are expected to be event-driven, which is in line with the serverless model discussed in our paper. Lambda@Edge\footnote{http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html} is a new functionality of AWS that allows one to explicitly deploy lambda functions to certain (coarse-grained) edge locations. The functionality covered by Lambda@Edge is still limited to a few scenarios, all of them tied to CloudFront (the AWS content delivery network) events. EdgeScale~\cite{Lara2016hierarchical}, by Huawei, implements serverless functions, storage, routing and additional capabilities from scratch (while we opted for current open technologies such as IBM/Apache Openwhisk) in a hierarchy of data centers, over a network between the user and traditional wide-area cloud providers. EdgeScale is on an early stage and does not report any empirical evaluation of latency, throughput, bandwidth and cost.

%

%according to their CloudFront content delivery service, which consists of approximately 90 edge nodes worldwide. However,  In contrast, we consider the continuum to be fine-grained, with nearby domains distributed one every $km^2$ or less. Furthermore, the upcoming  small 5G cells and microcells~\cite{beck2014mobile} allow us to think of one local-edge domain per block, or even per building in certain vital places, such as government buildings or transport stations.








%\subsection{Edge and Mobile Edge}
%The work in~\cite{beck2014mobile} provides technical details of a real-world mobile-edge prototype platform (by Nokia and Intel), together with a taxonomy of applications that can leverage such a platform. In line with our work, mobile-edge servers on base stations are equipped with computational capabilities and application deployment is based on virtualization technologies. Applications running on the mobile edge are expected to be event-driven, as in the serverless model. Moreover, identified use cases encompass offloading and local connectivity, both aspects covered by A3-E. 

%Ismail et al.~\cite{ismail2015icos} evaluated different aspects of the deployment and operation of a container technology locally on edge nodes. In their work, a testbed was setup using a database and three edge nodes interconnected by a company network. Despite the similarity with this work, our proposal goes beyond virtualization and containerization of application logic, in favor of serverless computing to optimize the use of edge resources.

%Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec} or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.

%mitigates the overload in MEC servers by deploying a serverless architecture on them, which provides an effective and efficient usage of available resources.  As we envision an heterogeneous continuum in which edge nodes may not be aware of each other (e.g., a domestic edge node may not be connected with a mobile one).

%. One recovery scheme is where an overloaded MEC server offloads its work to available neighbors within transfer range. The other recovery scheme is for situations when there is no available neighboring MEC within transfer range, and uses devices as ad-hoc relay nodes in order to bridge two MEC servers. 

%\subsection{Serverless Architectures}

%IBM OpenWhisk is an open source experimental FaaS started in 2016. It provides a programming model to upload event handlers to a cloud service, and register the handlers to respond to various events or direct invocations from web/mobile apps or other endpoints.

%Google Cloud Function is another service started in 2016 and still in alpha-release. Functions can be triggered by any Google service that supports cloud  pub/sub, cloud storage, arbitrary web hooks or direct triggers.

%Microsoft Azure Functions supports integration with other Azure products, plus timers and arbitrary applications via webhooks or HTTP triggers, and on-premises via the service bus.

%Webtask.io supports scheduled tasks in addition to events, but as a stand-alone tool – those events arrive via webhooks and HTTP endpoints. Webtask.io is built atop CoreOS, Docker, etcd and fleet, with bunyan and Kafka for logging.

%Finally, OpenLambda is an academic effort towards an open-source serverless architecture~\cite{hendrickson2016serverless}, consist of a number of subsystems that coordinate to run Lambda handlers, including a local execution engine that sandboxes handlers, a load balancer, and a distributed database. 

%The idea of bringing serverless capabilities to the edge is very recent. The first documented efforts date form 2017, and come mostly from the industry. 

%\textcolor{blue}{ Gartner predicting that smartphones would be the Internet access device of choice by 2018 [25 IFOGSIM Gartner]. }  

%\textcolor{blue}{  }

%\textcolor{blue}{ Within this proposal, the \textbf{Serverless Stream Model} an extension of the traditional stream processing model. The transformation function is the core concept and encapsulates user-defined data analytics logic to process data along the stream. These functions are then composed into topologies that enable complex data processing applications.  The wrapper is responsible for encapsulating the transformation functions and exposing a thin API layer, enabling the analytics function layer to treat functions as \textbf{microservices}.  For stateful functions, these wrappers also provide implicit state management. The wrapper transparently handles state replication and migration, and access to a function’s state is controlled via the exposed API. }


%\textcolor{blue}{  ENORM framework, uses the following five components on the edge node:	Resource allocator keeps track of the available CPU cores and memory. Edge Manager is composed by the node manager (deals with the requests obtained by the server manager from a cloud server) and server manager (initializes containers, allocates ports and security). Monitor, since each application edge server is monitored periodically by tracking communication/computing latency (through pings and timestamps on server logs respectively). Auto-scaler dynamically allocates/deallocates hardware resources to the containers executing application servers. Finally, Application Edge Server is the actual server that consists on a partition of the cloud server, hosted on the edge node. } ENORM is particularly focused on resource management (top-down) from cloud to edge. We envision a bottom-up approach in which execution is firstly attempted at edge nodes and then going up in the hierarchy given the unfeasibility of running near to the devices.


