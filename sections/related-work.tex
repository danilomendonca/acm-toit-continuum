\section{Related Work}
\label{sec:related}

%The computational continuum
The first notion of a computational continuum was based on cloudlets (trusted, resource-rich computer or cluster of computers, well-connected to the Internet and available for nearby mobile devices~\cite{Satyanarayanan09cloudlets}) for a Mobile-Cloudlet-Cloud architecture capable of reducing response time~\cite{Soyata:2012}. The cloudlets are homogeneous and connected to each other to coordinate the offloading of computational intensive tasks. Static data (e.g., template images for face recognition) is stored in the cloudlets beforehand. Our approach encompasses intra-domain allocation (e.g., among cloudlets in the same domain), but also considers heterogeneous and disjoint domains among which a coordinated allocation is not feasible (e.g., among cloudlets from different providers). In the later case, allocation is performed dynamically by clients given the perceived QoS from different domains.

From a theoretical point of view,~\cite{sarkar2015assessment} compared the performance of the continuum (bridged by means of fog computing) and cloud computing in the context of IoT, showing that the continuum outperforms cloud computing as the fraction of applications demanding low-latency response increases. However, in scenarios where many applications do not have real-time response requirements, the continuum is observed to be an overhead compared to cloud-only execution.

More recently,~\cite{GuptaIfogSim17} proposed iFogSim, a toolkit for modeling and simulation of resource
management techniques in the continuum. It allows one simulate the placement of different modules of real-time applications to edge devices and then measure latency, network congestion, energy consumption, and cost. Similarly,~\cite{SonmezEgdeCloudSim17} proposed EdgeCloudSim, to simulate multi-tier scenarios where multiple edge servers are running in coordination with upper layer cloud solutions, also featuring a device mobility model and a load generator. Both simulation platforms extend the well-known cloud simulator framework CloudSim~\cite{calheiros2011cloudsim}. We opted for deploying real edge domains rather than simulating them, although simulation of mobile-edge scenarios is considered as a future work.

Nowadays, realizing edge computing is seen as the novel and challenging part of the continuum. In~\cite{Nastic17ServerlessDataAnalytics}, a unified cloud and edge data analytics platform is proposed. The later extends the notion of serverless computing to the edge via a reference architecture, enabling uniform development and operation of analytics functions, hiding away from the client the heterogeneity between cloud and edge. At the hearth of the model lays an orchestrator that performs a control loop. It receives
the application configuration directives, in terms of high-level objectives
such as optimizing network latency. It interprets and analyzes these goals
and decides how to orchestrate the
underlying resources, as well as the
user-defined functions, by invoking
the underlying runtime mechanisms. However, the concrete mechanisms for orchestration are left open. In contrast, the A3-E model tackles specific details regarding the autonomous self-management of FaaS-based microservices along the continuum. Additionally, this approach particularly targets data analytics applications, while our model focus on general purpose applications (particularly those with low latency requirements). Moreover, prototypes of A3-E middlewares have been implemented and evaluated. 

%This approach enables combining the benefits of the edge (lower response time and heterogeneous data management) with the computational and storage capabilities of the cloud. The proposed serverless data analytics paradigm is particularly suitable for managing different granularities of data analytics approaches bottom-up. This means that the edge focuses on local views (for example, per edge gateway), while the cloud supports global views, that is, combining and analyzing data from different edge devices, regions, or even domains. Data analytics can be performed on edge nodes, cloud nodes, or both, and delivered from any of the nodes directly to the application, based on the desired view. Moreover, the top-down control process allows decoupling of application requirements (the what) from concrete realization of those requirements (the how). This allows developers to simply define the analytics function behavior and data-processing business logic and application goals  instead of dealing with the complexity of different management, orchestration, and optimization processes.

%Computation offloading to the edge

~\cite{Satria2017mec} proposes concrete mechanisms for computation offloading among nodes located at neighbor base stations. In the context of A3-E model, this work provides a reactive domain-side allocation scheme targeting fault-tolerance in which mobile-edge domains are modeled as the set of servers comprised in neighbor base stations. Differently from our model, this work considers mobile devices as means of relaying data from disconnected base stations. In A3-E, the disconnected computational resources are considered as different domains. However, a client-side selection is responsible for reallocating domains based on their availability and provided QoS. 

%either directly or using nearby devices as relay nodes, as a solution for overloaded edge nodes. The former is where a node offloads its work to available neighbors within transfer range, while the latter is where no neighbor nodes within transfer range are available, and uses devices as ad-hoc relay nodes in order to bridge two edge servers. In contrast, our approach mitigates the overload in the edge by first, a serverless architecture that provides an effective and efficient usage of available resources, and second by sensing the QoS in the client-side middleware to decide whether to offload the computation to the edge/cloud. Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec}.

%In a similar direction, T\"arneberg et al.~\cite{Tarneberg2017} proposed a model that bridges mobile edge computing and the distributed cloud paradigm, as well as an algorithm to solve the resource management challenges that emerge from this integration. I or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.

In a similar direction,~\cite{Tarneberg2017} proposes an algorithm for application placement and resource management across an heterogeneous topology comprised of cloud and edge computing. 
%This approach stresses the challenges for such an heterogeneous topology as meeting all client applicationsâ€™ SLAs, minimizing infrastructure-wide operational expenditure and load balance of resources and mitigate resource usage skewness. 
The algorithm solves a multi-optimization problem considering application
execution cost and the overload penalty on each node in terms of latency, as well as edge
resources in the system. The objective function is constructed
from the infrastructure provider's viewpoint (i.e., Telecom operators), whose objective is to
minimise the overall running cost~\cite{weber2017facilitating}. We also envision an heterogeneous continuum, though we consider autonomous and independent \textit{domains} that cannot communicate and coordinate resource allocation. The proposed placement algorithm complements A3-E, as it could be employed by providers of multiple domains to optimize their costs (e.g., by restricting the availability of resources at different mobile-edge domains in certain conditions). In turn, A3-E tackles the adaptation to the conditions imposed by providers (e.g., with the deallocating of low-priority microservices from these domains and the consequent client-side selection of alternative domains).

%be seen as a first step to decide whether to use proactive or reactive allocation for each domain and application. From then on, the availability also depends on what is perceived by the clients, thus the placement is coordinated through the middleware. 

The Enorm framework~\cite{wang2017enorm} for edge node resource management enables the use of the cloud in conjunction with edge by deploying a cloud server manager on each application server. This server communicates with potential edge nodes requesting computing services, and
deploys partitioned servers on the edge node, and receives updates from the edge node to update the global view of the application server on the cloud. Each edge node maintains local data, relevant to the users on its coverage area. The global view is then maintained on the cloud and updated periodically by edge nodes. When edge nodes cannot provide computing services nor improve the QoS of the application, users connect to the cloud application server as in a typical cloud scenario. The allocation follows an hierarchical approach, with a centralized control starting from the cloud and spreading to the edge nodes. In turn, A3-E offers a decentralized approach in which autonomous clients and domains are able to coordinate the placement of computation along a mobile-edge-cloud continuum. 

%, without depending necessarily on the cloud to decide the offloading. %Although the data management aspect on Enorm can complement our approach, it does not consider 
%and can be complementary to our work by addressing data locality and replication problems at architectural level sinc
%For this reason, offloading is dynamically decided by A3-E on the client side according to the perceived QoS of the available domains. 
%\textcolor{blue}{[Gupta IfogSim]}


From the industry point of view, the first real-world prototype of the mobile edge~\cite{beck2014mobile} (by Nokia Siemens and Intel~\cite{NokiaMEC13}) features base stations equipped with commodity hardware, and application deployment is based on virtualization and containerization technologies~\cite{ismail2015icos}.  
Applications running on the mobile edge are expected to be event-driven, which complements the A3-E model that currently relies on request/response RESTful calls. However, this approach does not take into account the inherent resource limitations of edge computing, tackled by A3-E by adhering to the FaaS execution model.
%This industry prototype suggests that end users
%benefit mostly from reduced communication delay, benefiting computation intensive applications by means of offloading (as shown throughout this paper). Meanwhile, Telecom providers benefit from bandwidth consumption reduction and better scalability, while application providers profit with respect to faster
%services.

Finally, Lambda@Edge\footnote{http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html} is a new functionality of AWS that allows one to explicitly deploy lambda functions to certain (coarse-grained) edge locations. The functionality covered by Lambda@Edge is still limited to a few scenarios, all of them tied to CloudFront (the AWS content delivery network) events. 
%In turn, A3-E deals with heterogeneous and general purpose applications and is not tied to any particular source of events.
%, that is, when a user request to see a given content, when the request is forwarded to the cloud (because such content was not available at the edge location), and vice versa for the response.

%EdgeScale~\cite{Lara2016hierarchical}, by Huawei, also brings the notion of serverless to the edge. It implements serverless functions, storage, routing and additional capabilities from scratch (while we opted for current open technologies such as Apache Openwhisk) in a hierarchy of edge data centers, over a network between the user and traditional wide-area cloud providers. Once again, this proposal does not consider independent (and potentially disjoint) domains. Besides, EdgeScale is on an early stage and does not report any empirical evaluation of latency, throughput, bandwidth and cost.

%

%according to their CloudFront content delivery service, which consists of approximately 90 edge nodes worldwide. However,  In contrast, we consider the continuum to be fine-grained, with nearby domains distributed one every $km^2$ or less. Furthermore, the upcoming  small 5G cells and microcells~\cite{beck2014mobile} allow us to think of one local-edge domain per block, or even per building in certain vital places, such as government buildings or transport stations.








%\subsection{Edge and Mobile Edge}
%The work in~\cite{beck2014mobile} provides technical details of a real-world mobile-edge prototype platform (by Nokia and Intel), together with a taxonomy of applications that can leverage such a platform. In line with our work, mobile-edge servers on base stations are equipped with computational capabilities and application deployment is based on virtualization technologies. Applications running on the mobile edge are expected to be event-driven, as in the serverless model. Moreover, identified use cases encompass offloading and local connectivity, both aspects covered by A3-E. 

%Ismail et al.~\cite{ismail2015icos} evaluated different aspects of the deployment and operation of a container technology locally on edge nodes. In their work, a testbed was setup using a database and three edge nodes interconnected by a company network. Despite the similarity with this work, our proposal goes beyond virtualization and containerization of application logic, in favor of serverless computing to optimize the use of edge resources.

%Certainly, the scalability of our proposed architecture could be extended by means of a neighbor offloading strategy as proposed in~\cite{Satria2017mec} or by an integration of MEC and cloud resources as proposed in ~\cite{Tarneberg2017}.

%mitigates the overload in MEC servers by deploying a serverless architecture on them, which provides an effective and efficient usage of available resources.  As we envision an heterogeneous continuum in which edge nodes may not be aware of each other (e.g., a domestic edge node may not be connected with a mobile one).

%. One recovery scheme is where an overloaded MEC server offloads its work to available neighbors within transfer range. The other recovery scheme is for situations when there is no available neighboring MEC within transfer range, and uses devices as ad-hoc relay nodes in order to bridge two MEC servers. 

%\subsection{Serverless Architectures}

%IBM OpenWhisk is an open source experimental FaaS started in 2016. It provides a programming model to upload event handlers to a cloud service, and register the handlers to respond to various events or direct invocations from web/mobile apps or other endpoints.

%Google Cloud Function is another service started in 2016 and still in alpha-release. Functions can be triggered by any Google service that supports cloud  pub/sub, cloud storage, arbitrary web hooks or direct triggers.

%Microsoft Azure Functions supports integration with other Azure products, plus timers and arbitrary applications via webhooks or HTTP triggers, and on-premises via the service bus.

%Webtask.io supports scheduled tasks in addition to events, but as a stand-alone tool â€“ those events arrive via webhooks and HTTP endpoints. Webtask.io is built atop CoreOS, Docker, etcd and fleet, with bunyan and Kafka for logging.

%Finally, OpenLambda is an academic effort towards an open-source serverless architecture~\cite{hendrickson2016serverless}, consist of a number of subsystems that coordinate to run Lambda handlers, including a local execution engine that sandboxes handlers, a load balancer, and a distributed database. 

%The idea of bringing serverless capabilities to the edge is very recent. The first documented efforts date form 2017, and come mostly from the industry. 

%\textcolor{blue}{ Gartner predicting that smartphones would be the Internet access device of choice by 2018 [25 IFOGSIM Gartner]. }  

%\textcolor{blue}{  }

%\textcolor{blue}{ Within this proposal, the \textbf{Serverless Stream Model} an extension of the traditional stream processing model. The transformation function is the core concept and encapsulates user-defined data analytics logic to process data along the stream. These functions are then composed into topologies that enable complex data processing applications.  The wrapper is responsible for encapsulating the transformation functions and exposing a thin API layer, enabling the analytics function layer to treat functions as \textbf{microservices}.  For stateful functions, these wrappers also provide implicit state management. The wrapper transparently handles state replication and migration, and access to a functionâ€™s state is controlled via the exposed API. }


%\textcolor{blue}{  ENORM framework, uses the following five components on the edge node:	Resource allocator keeps track of the available CPU cores and memory. Edge Manager is composed by the node manager (deals with the requests obtained by the server manager from a cloud server) and server manager (initializes containers, allocates ports and security). Monitor, since each application edge server is monitored periodically by tracking communication/computing latency (through pings and timestamps on server logs respectively). Auto-scaler dynamically allocates/deallocates hardware resources to the containers executing application servers. Finally, Application Edge Server is the actual server that consists on a partition of the cloud server, hosted on the edge node. } ENORM is particularly focused on resource management (top-down) from cloud to edge. We envision a bottom-up approach in which execution is firstly attempted at edge nodes and then going up in the hierarchy given the unfeasibility of running near to the devices.


