\section{Experimental Evaluation}\label{sec:evaluation}

%\subsection{Domains Setup}

%\subsubsection{Domains}

In order to evaluate our approach, we deployed a set of domains to materialize the computational continuum --- as shown in Table~\ref{tab:domain-exp-config}. The sample application is a simplified augmented reality application, which allows users to capture an image and then invoke a service (deployed as functions along the continuum) for feature extraction and matching upon that image. 

 As a mobile domain we used a smartphone in which we deployed the A3-E middleware and the functions for feature extraction and matching running locally. Local-edge domains feature the IBM/Apache Openwhisk  serverless framework\footnote{https://openwhisk.incubator.apache.org/} that manages {\em actions} (equivalent to functions in openwhisk jargon). Being open-source, openwhisk is (to date) the only serverless alternative among the major vendors that can be deployed locally or on private clouds. 
%Particularly, openwhisk provides a built-in noSQL database: CouchDB, which is associated with the implemented actions through user-defined triggers and rules. 
Both edge-local domains are placed in the same LAN as the origin of the requests (i.e., the client applications/devices), to emulate the few-hop scenario in which devices are directly connected to their nearest Edge domains. Edge-local-1 is placed on a regular laptop, allowing us to represent a situation where latency is close to zero, but the computational resources are highly constrained, as scaling-up is not possible due to inherent physical restrictions of the underlying infrastructure. Similarly, edge-local-2 domain is placed on an university server, where the computational resources are less constrained, and still low latency can be achieved due to physical proximity and data locality.

\begin{table}[htb]
	\caption{Domains Setup in the Continuum for the Experimental Evaluation}
	\label{tab:domain-exp-config}
	\begin{tabular*}{1\textwidth}{@{\extracolsep{\fill}}>{\raggedright}p{1.5cm}>{\raggedright}p{6cm}>{\raggedright}p{6cm}}
		\toprule 
		Domain & Machine Resources & Execution Environment\tabularnewline
		\midrule
		\midrule 
		Mobile & Samsung Galaxy S6 SM-G90, 3Gb RAM, 8x Cortex CPU 2Ghz & Android 5.0.2 + Java Functions + OpenCV
		\tabularnewline
		\midrule 
		Local-edge-1  & ubuntu/trusty64-2, 4x vCPUs, 4Gb RAM & Openwhisk, 256 Mb/Action, Python 2.7 + OpenCV \tabularnewline
		\midrule 
		Local-edge-2  & ubuntu/trusty64-2, 8x vCPUs, 16Gb RAM & Openwhisk, 256 Mb/Action, Python 2.7 + OpenCV \tabularnewline
		\midrule 
		Cloud-FaaS & N/A & AWS Lambda, 256 Mb/Function, Python 2.7 + OpenCV \tabularnewline
		\midrule 
		Cloud-IaaS & Autos Scaling Group with t2.micro instances + Amazon Linux AMI 2017  & NodeJs 6.11 server + Python 2.7 + OpenCv
		\tabularnewline
		\bottomrule
	\end{tabular*}
\end{table}


%In this experiment, we considered two alternatives for deploying the serverless continuum architecture, mimicking the behavior of both an edge node and a fog node (Figure~\ref{fig:exp-edge}).  

%The client application is embedded in the edge node, consisting on the postman requests and the node.js endpoint (Figure~\ref{fig:exp-setup1}). 
%On the edge-local alternative, we used a virtual machine running locally, on a regular laptop, with 4x CPU, 4x Gb of RAM and 40 Gb SSD of storage. 

%For the Fog alternative, we deployed the serverless architecture on Policloud\footnote{http://policloud.polimi.it/}, the private IaaS solution of Politecnico di Milano. Here, the computational resources are less constrained, and still low latency can be achieved due to physical proximity (two hops from the client) and data locality. This setup runs on a small cluster of 4 virtual machines with 2x CPU, 4x Gb of Ram and 100 GB SSD, each running a different component of openwhisk (triggers and storage, Http server, controller, and invokers, respectively). Note that in this case the fog node is deployed in the same LAN that originates the requests, to emulate the few-hop scenario in which devices are directly connected to their corresponding MEC.

The serverless cloud alternative (Cloud-FaaS) for this experiment uses AWS Lambda\footnote{https://aws.amazon.com/lambda/} as the first-available and most mature serverless solution in the market. The functions and associated libraries and services (storage, feature extraction and matching) are hosted in the same region, which is enforced by AWS to guarantee a certain degree of data locality. Finally, we also deployed the functions in a typical ``serverful'' Cloud setup (Cloud-IaaS). However, the main goal of this experiment is not to compare traditional cloud services against a serverless solution, but to demonstrate that the proposed continuum can outperform the Cloud under the tested circumstances and requirements.


\subsection{Domain Latency Evaluation} 

In order to compare the latency of the different domains along the continuum, we performed a first experiment with different number of parallel clients, which perform 100 requests each, at a constant rate of two per second. Each request consisted on feature extraction and matching from a sample image. This setup considers not only the default maximum for concurrent executions in AWS Lambda\footnote{http://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html} and Openwhisk\footnote{https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md}, but also the limited resources of the edge domains. 

This experiment was performed without using the A3-E middleware nor the device (mobile domain), since it aims to evaluate the plain latency for remote domains when processing simultaneous requests. This scenario would not occur in a mobile domain, which only process local requests. The possibility of offloading computation from one mobile device to another is left as a future work.

Figure~\ref{fig:exp-setup1} shows the experimental setup for evaluating the remote domains. Capturing and uploading an image is emulated using Postman\footnote{https://www.getpostman.com/}, a JavaScript open source application designed to load test functional behaviors and measure the performance of Web APIs. The  payload for this experiment was a sample image of approximately 65 Kb, which is a reasonable size for this use case considering the requirements regarding low-latency and computation time~\cite{rodriguez16mobile}. 

Then, the image is sent through HTTP/POST and different subsequent steps are executed depending on the domain: triggering Openwhisk actions for both edge domains, AWS Lambda functions for Cloud-FaaS domain, and a Nodejs server that calls a Python function for Cloud-IaaS domain.

%In our edge domains for the experiment, uploading an image to CouchDB (Step 3.a) triggers the action that performs the feature extraction and matching (Step 4.a)  with the points-of-interest, supported by the OpenCV\footnote{\url{http://opencv.org}} visual recognition library (Step 5.a). 


\begin{figure}[htb]
	
	\centering
	\includegraphics[width=0.9\textwidth]{figs/experimental-setup.pdf}
	\caption{Setup for Latency Experiments}
	\label{fig:exp-setup1}
\end{figure}

%\subsubsection{Baseline Latency}

 Figure~\ref{fig:latency-domains} shows the average latency for each scenario, averaged through 5 executions. Note that the function computation time (light gray) is distinguished from the overhead (dark gray), which includes network communication (routing, forwarding) and queuing time (when no resources are available to process the request immediately). 

\begin{figure}
	
	\centering
	\includegraphics[width=1\textwidth]{figs/latency-domains}
	\caption{Latency results for each domain and different number of clients.}
	\label{fig:latency-domains}
\end{figure}

 For these scenarios, the latency added by the edge domains is less than the latency in both cloud alternatives, considering up to 16 simultaneous clients. Regarding the Cloud-Faas domain, the latency reduction is up to 90\% for edge-local-1 and up to 82\% for edge-local-2 respectively. Regarding the traditional cloud domain, reductions are up to 77\% and 58\% respectively. Since in Cloud-IaaS a scaling-up action (that can demand several seconds~\cite{Quatrocchi2016discrete}) triggers when more than one VM instance is needed to handle the workload, requests timeout in the meantime. 
 
 For light to medium workloads, the edge-local domains outperformed all the other alternatives for this scenario. However, edge-local-1 is the most resource-constrained, which hinders its availability under heavier workloads. Interestingly, the Cloud-IaaS domain also outperformed the serverless one (46\% less latency) for light workloads. This can be due to the additional steps performed by the API Gateway in order to forward RESTful calls to lambda functions in Cloud-FaaS\footnote{http://docs.aws.amazon.com/lambda/latest/dg/with-on-demand-https.html}. Nevertheless, this advantage is mitigated by the fact that Cloud-FaaS is a serverless alternative, more reactive against bursts of workload by scaling-up faster and scaling-down to zero when functions are not used~\cite{Villamizar2017lambda,Hendrickson:2016}.


%\subsection{Battery} The second set of experiments targeted the measurement of battery consumption of a mobile device in two scenarios: 1) in which feature extraction and matching were performed locally, and 2) these tasks were offloaded to edge servers.

\subsection{Evaluation of A3-E in the Continuum} 

The next set of experiments targeted the evaluation of A3-E within the computational continuum. In specific, the setup consisted of a mobile device hosting the A3-E Middleware in charge of dynamically allocating the best domain available for execution, based on the perceived QoS, as described in Section~\ref{sec:proposal}. The sample application was the  augmented reality application, with feature extraction and matching modeled as functions that can be executed locally (Java Functions), in an edge domain (implemented with OpenWhisk), or a cloud-FaaS domain (AWS Lambda). The experiment featured four different scenarios. The first three scenarios are simple ones, where only one domain is available: (1) the mobile device, (2) edge-local domain, or (3) cloud-faas domain. The fourth scenario brings together the three previous domains as a continuum, allowing to allocate any of them by means of A3-E. 

For the last scenario, the availability of different domains was modeled probabilistically, with the following considerations. The average unavailability of the cloud domain~\cite{garcia2017bandwidth} is once every 15 minutes, e.g., because the device is outside mobile coverage area. A3-E was configured to ping for domain availability and perceived QoS every two seconds ($u = 2$). Using a geometric distribution\footnote{https://math.stackexchange.com/questions/165993/average-number-of-times-it-takes-for-something-to-happen-given-a-chance}, the probability of pinging and the cloud being unavailable is: $internetOnToOff=0.022$. Once the cloud is unavailable, the average time for becoming available again is 2 minutes, i.e., probability $internetOffToOn=0.166$. The rationale for the edge domain is analogous, but considering a higher probability of being unavailable --- e.g., due to lack of physical proximity to edge servers. In this case, the edge is unavailable once every 10 minutes: $edgeOnToOff = 0.33$. Once unavailable, we considered an average of 5 minutes for becoming available again: $edgeOffToOn=0.066$. Finally, the mobile device is considered to be always available, but is selected only when the other domains are unavailable, or the sensed latencies are too high for the application requirements.


\begin{comment}
internetOnToOffProbability = 0.22%
internetOffToOnProbability = 1.66%

edgeOnToOffProbability = 3.33%;
edgeOffToOnProbability = 6.66%;

e = 1/p; //average number of trials it takes for the event with probability p to happen
~t = e * u;  //average time it takes for the event with probability p to happen given a trial interval u
~t = u/p; //the same, but using the trial interval and the probability p 

###

trial interval u = 2 seconds

average time for disconnecting: 15 x 60 seconds
prob. of disconnecting: 0.2222222%   

average time for reconnecting: 2 x 60 seconds
prob. of reconnecting: 1.666666%

average time for edge unavailability: 10 x 60 seconds 
prob. of edge unavailability: 0.333333%

average time for edge becoming available: 5 x 60 seconds 
prob. of edge becoming available: 0.666666%

\end{comment}

%(all-domains).

The experiment execution consisted on cascading 2000 sequential requests for feature extraction and matching of a sample image (with a size of 65 Kb). Then, we measured total execution time, battery consumption in the mobile device, and average time per call. Figure~\ref{fig:exp-a3e} shows the experimental results, averaged among 5 executions for each scenario.
 For the total execution time (Fig.~\ref{fig:total-exec-a3e}), considering the Cloud-FaaS domain as baseline, Edge-local improved the execution time up to a 72\%, while mobile and all-domains up to a 69\%  and 49\% respectively. The highest battery consumption (Fig.~\ref{fig:battery-a3e}) was when using only the mobile-device domain, with a battery drop of 4.5\% during 750 seconds (12.5 minutes) of execution. Battery savings with Cloud-FaaS, Edge-local and All-domains were 49\%, 35\% and 49\%, respectively. The average execution time (Fig.~\ref{fig:time-per-call-a3e}), with Cloud-FaaS as baseline (1137 milliseconds per call), was improved 76\%, 68\% and 47\% for Edge-local, Mobile-device and All-domains respectively. Finally, for All-domains scenario, Fig.~\ref{fig:calls-per-domain-a3e} shows the average number of calls that were processed by each domain in the continuum, with 65\% by the edge-local domain, followed by Cloud-FaaS (30\%) and Mobile-device (7\%).
 
From this experiment we can highlight the following aspects. First, the total execution time when using only the cloud was two times higher than when using the continuum (all-domains) and A3-E to select among available domains. This is because the requests were performed in cascade, and given the higher latency per call in the cloud, the total time increases accordingly. Clearly, switching to edge domains (when available) by means of A3-E substantially reduces the latency and improves perceived QoS.

Even though, the battery consumption was substantially lower when offloading computation rather than performing it on the device. The mobile-device scenario lasted half of the time but used twice as battery (a prohibitive 20\% of battery drain per hour) as the all-domains one, given that it was performing CPU intensive operations. This recalls the importance of computation offloading to preserve devices' resources.

Finally, the domain selection and distribution of computation performed by A3-E in the all-domains scenario reflects the underlying idea of the computational continuum: exploit as much as possible edge domains when available (by giving them high scores in Formula~\ref{eq:smart}) to achieve the better balance among computation time and resource consumption. Then, switch to the cloud or to the local device only when necessary. Note that in this experiment the mobile-device computation was given the lowest possible score, thus the cloud was selected as the first alternative to edge domains. This behavior can be tunned by adjusting the application requirements and domain parameters discussed in Section~\ref{sec:implementation}.

%that allows to cope with unavailability and fluctuations in QoS 

%\textcolor{blue}{TODO:(2) Discuss the relevance of the results }


\begin{figure}[tbp]
	\raggedright
	\subfloat[Total execution time (seconds)\label{fig:total-exec-a3e}] {\includegraphics[width=0.5\textwidth]{figs/total-exec-time-A3E}}
	\subfloat[Battery consumption (\%)\label{fig:battery-a3e}] {\includegraphics[width=0.5\textwidth]{figs/battery-consumption-A3E}}
	
	\subfloat[Execution time per call (miliseconds)\label{fig:time-per-call-a3e}] {\includegraphics[width=0.5\textwidth]{figs/time-per-call-A3E}}
	\subfloat[Number of Calls per domain in all-domains scenario\label{fig:calls-per-domain-a3e}] {\includegraphics[width=0.5\textwidth]{figs/calls-per-domain-A3E}}
	
	\caption{A3-E experimental evaluation results} \label{fig:exp-a3e}
\end{figure}





